{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/data_analytics/Week_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Import only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i loved the smell of Flowers in the Room. i loved the smell of Rain in my Chamber. i always despised the taste of ginger Lily Candles in that House.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = 'i loved the smell of Flowers in the Room.'\n",
    "d2 = 'i loved the smell of Rain in my Chamber.'\n",
    "d3 = 'i always despised the taste of ginger Lily Candles in that House.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i loved the smell of flowers in the room. i loved the smell of rain in my chamber. i always despised the taste of ginger lily candles in that house.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61fb8538",
   "metadata": {},
   "source": [
    "hier werden die Grossbuchstaben in Kleinbuchstaben umgewandelt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i loved the smell of flowers in the room i loved the smell of rain in my chamber i always despised the taste of ginger lily candles in that house'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c403d2fb",
   "metadata": {},
   "source": [
    "komma und Punkte und Zeichen werden entfernt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'about', 'out', 'a', 'ours', 'their', 'having', 'what', 'some', 'yours', 'mustn', 'just', 'can', 'who', 'against', 'on', 'all', 'that', 'by', \"don't\", 'yourselves', 'at', 'whom', 'm', \"mightn't\", \"needn't\", 'aren', 'hasn', 'was', 'ma', 'd', \"weren't\", 'did', 'before', 'both', 'weren', 'isn', 'with', 'more', 'than', 'o', \"shan't\", 'how', 'me', 'doesn', 'from', 'i', 'to', \"mustn't\", \"won't\", 'him', 'will', \"shouldn't\", \"isn't\", 're', 'should', 'any', 'her', \"hasn't\", 'if', 'now', 'most', 'an', 'of', 'don', 'himself', 'once', 'until', \"you'll\", 'the', 'y', 'were', 'my', 'theirs', 'too', 'why', 'needn', \"you'd\", 'there', 'very', 'own', 'been', 'shouldn', 'ourselves', 'again', 'herself', 'his', 'mightn', 'being', 'only', 'they', 'didn', 'down', 'our', \"hadn't\", 'wouldn', \"you're\", 'those', 'you', 'same', 'it', 'and', 'while', 'yourself', 'but', 'here', 'had', 'am', 'its', 'we', 'such', \"wasn't\", 't', 'between', 'no', 'won', 'not', 'above', 'or', 'are', 'which', 'for', 'shan', 'so', \"it's\", 'when', 'she', 'where', 'nor', 'is', 'hers', 'over', 'has', 'itself', 'your', 'll', 'during', 'after', 'ain', 'have', 'in', \"aren't\", 'this', \"she's\", 'each', 'under', 'into', 'off', 've', \"that'll\", 'up', 'few', 'as', 'do', 'hadn', 'themselves', \"you've\", 'be', \"should've\", 'other', 'haven', 'myself', 'these', 'because', 'does', \"couldn't\", 'them', 'wasn', \"wouldn't\", 'through', 'further', 's', 'he', \"doesn't\", 'below', 'couldn', 'doing', \"didn't\", 'then', \"haven't\"}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loved', 'smell', 'flowers', 'room', 'loved', 'smell', 'rain', 'chamber', 'always', 'despised', 'taste', 'ginger', 'lily', 'candles', 'house']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9e423d",
   "metadata": {},
   "source": [
    "die stopwords werden entfernt -> hier i, the, of"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['loved', 'smell', 'flowers', 'room', 'loved', 'smell', 'rain', 'chamber', 'always', 'despised', 'taste', 'ginger', 'lily', 'candles', 'house'] \n",
      "\n",
      "After lemmatization:\n",
      "['love', 'smell', 'flower', 'room', 'love', 'smell', 'rain', 'chamber', 'always', 'despise', 'taste', 'ginger', 'lily', 'candle', 'house']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb778da",
   "metadata": {},
   "source": [
    "alle unnötigen Enden werden entfernt. ed, "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "corpus = ['love smell flower room', \n",
    "          'love smell rain chamber', \n",
    "          'despise taste ginger lily candle house']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e982962",
   "metadata": {},
   "source": [
    "jetzt haben wir die sätze nach der lemmatization eingefügt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   candle  chamber  despise  flower  ginger  house  lily  love  rain  room  \\\n",
      "0       0        0        0       1       0      0     0     1     0     1   \n",
      "1       0        1        0       0       0      0     0     1     1     0   \n",
      "2       1        0        1       0       1      1     1     0     0     0   \n",
      "\n",
      "   smell  taste  \n",
      "0      1      0  \n",
      "1      1      0  \n",
      "2      0      1  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2632c9",
   "metadata": {},
   "source": [
    "## erklärung\n",
    "\n",
    "Der CountVectorizer zählt, wie oft jedes Wort in jedem Satz vorkommt. Danach entsteht eine Tabelle (Dokument-Term-Matrix)\n",
    "\n",
    "Zeilen: Jeder Satz in deiner Sammlung.\n",
    "Spalten: Jedes einzelne Wort, das in den Sätzen vorkommt.\n",
    "Werte: Wie oft das Wort in dem jeweiligen Satz vorkommt.\n",
    "\n",
    "Dies hilf die Sätze in numerischen Darstellung zu erstellen. diese kann man dann für weitere Analysen gebrauchen. \n",
    "\n",
    "\n",
    "### ngram_range=(1,1)\n",
    "N-Gramme: N-Gramme sind Kombinationen von \"n\" Wörtern, die zusammen auftreten. Zum Beispiel:\n",
    "Unigramme (n=1): Einzelne Wörter (z. B. \"Liebe\", \"Blume\")\n",
    "Bigramme (n=2): Kombinationen von 2 Wörtern (z. B. \"Liebe Blume\")\n",
    "Trigramme (n=3): Kombinationen von 3 Wörtern (z. B. \"Liebe die Blume\")\n",
    "\n",
    "Im Fall von ngram_range=(1,1) bedeutet es, dass wir nur einzelne Wörter betrachten also Unigramme.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   candle house  despise taste  flower room  ginger lily  lily candle  \\\n",
      "0             0              0            1            0            0   \n",
      "1             0              0            0            0            0   \n",
      "2             1              1            0            1            1   \n",
      "\n",
      "   love smell  rain chamber  smell flower  smell rain  taste ginger  \n",
      "0           1             0             1           0             0  \n",
      "1           1             1             0           1             0  \n",
      "2           0             0             0           0             1  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d149fc",
   "metadata": {},
   "source": [
    "## erklärung\n",
    "jetzt haben wir ngram_range=(2,2)\n",
    "wir suchen / finden nun Pärchen. Die Spalten repräsentieren nun nicht mehr einzelne Wörter sondern Paare von Wörter, die in den Sätzen vorkommen.\n",
    "\n",
    "Zeilen: Repräsentieren die einzelnen Sätze.\n",
    "Spalten: Repräsentieren die Bigramme (Pärchen von Wörtern) aus den Sätzen.\n",
    "Werte: Zeigen, wie oft jedes Bigramm in jedem Satz vorkommt.\n",
    "\n",
    "also Paare wurden gebildet und in die Matrix wird mit 1 oder 0 angegeben, ob diese Paare in den 3 Sätzen vorkommen. \n",
    "\n",
    "Gibt detailliertere Analyse der Kombinationen von Wörtern, die gemeinsam auftreten, hilft um die Bedeutung der Wörter zu ermitteln."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 12 \n",
      "\n",
      "The words in the corpus: \n",
      " {'flower', 'lily', 'ginger', 'chamber', 'smell', 'rain', 'love', 'house', 'room', 'despise', 'taste', 'candle'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "   flower    lily  ginger  chamber  smell  rain  love   house  room  despise  \\\n",
      "0    0.25  0.0000  0.0000     0.00   0.25  0.00  0.25  0.0000  0.25   0.0000   \n",
      "1    0.00  0.0000  0.0000     0.25   0.25  0.25  0.25  0.0000  0.00   0.0000   \n",
      "2    0.00  0.1667  0.1667     0.00   0.00  0.00  0.00  0.1667  0.00   0.1667   \n",
      "\n",
      "    taste  candle  \n",
      "0  0.0000  0.0000  \n",
      "1  0.0000  0.0000  \n",
      "2  0.1667  0.1667  \n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d46e52",
   "metadata": {},
   "source": [
    "misst jetzt wie jäufig ein Wort in den einzelnen Sätzen vorkommt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "         flower:     0.4771\n",
      "           lily:     0.4771\n",
      "         ginger:     0.4771\n",
      "        chamber:     0.4771\n",
      "          smell:     0.1761\n",
      "           rain:     0.4771\n",
      "           love:     0.1761\n",
      "          house:     0.4771\n",
      "           room:     0.4771\n",
      "        despise:     0.4771\n",
      "          taste:     0.4771\n",
      "         candle:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea41b87e",
   "metadata": {},
   "source": [
    "misst jetzt wie wichtig ein Wort in der gesamten MAtrix ist\n",
    "\n",
    "\n",
    "Ein Wort, das in vielen Dokumenten vorkommt, hat eine geringe Inverse Document Frequency, weil es weniger informativ ist. Ein Wort, das nur in wenigen Dokumenten vorkommt, hat eine hohe IDF, weil es spezifisch und damit informativer ist.\n",
    "\n",
    "hier zum beispiel: smell -> ist tiefer, weil es öfter vorkommt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "   flower    lily  ginger  chamber  smell    rain   love   house    room  \\\n",
      "0  0.1193  0.0000  0.0000   0.0000  0.044  0.0000  0.044  0.0000  0.1193   \n",
      "1  0.0000  0.0000  0.0000   0.1193  0.044  0.1193  0.044  0.0000  0.0000   \n",
      "2  0.0000  0.0795  0.0795   0.0000  0.000  0.0000  0.000  0.0795  0.0000   \n",
      "\n",
      "   despise   taste  candle  \n",
      "0   0.0000  0.0000  0.0000  \n",
      "1   0.0000  0.0000  0.0000  \n",
      "2   0.0795  0.0795  0.0795  \n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6398d1f",
   "metadata": {},
   "source": [
    "ist nun der Wert, der die Bedeutung eines Worts im Kontext der Sätze und der ganzen MAtrix bewertet. Ein Wort, das in allen Sätzen vorkommt, hat keinen Informationswert, weil es nicht hilft, zwischen den Sätzen zu unterscheiden.\n",
    "\n",
    "TF-IDF kombiniert die beiden vorhin erhaltenen Matrizen, um zu bewerten, wie bedeutend ein Wort ist im Satz im vergleich zu den anderen Sätzen.\n",
    "\n",
    "jetzt merkt man, dass so individuelle wörter wie chamber, flower oder room am höchsten gewertet werden. hingegen smell und love viel tiefer, da es in fast allen Sätzen vorkommt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Regulators', 'NNS', 'O'),\n",
      " ('imposed', 'VBD', 'O'),\n",
      " ('a', 'DT', 'O'),\n",
      " ('$', '$', 'O'),\n",
      " ('4.8', 'CD', 'O'),\n",
      " ('billion', 'CD', 'O'),\n",
      " ('penalty', 'NN', 'B-NP'),\n",
      " ('on', 'IN', 'O'),\n",
      " ('Apple', 'NNP', 'O'),\n",
      " ('Tuesday', 'NNP', 'O'),\n",
      " ('for', 'IN', 'O'),\n",
      " ('violating', 'VBG', 'O'),\n",
      " ('antitrust', 'JJ', 'O'),\n",
      " ('laws', 'NNS', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('App', 'NNP', 'O'),\n",
      " ('Store', 'NNP', 'O'),\n",
      " ('policies', 'NNS', 'O'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = '''Regulators imposed a $4.8 billion penalty on Apple Tuesday for violating antitrust laws in its App Store policies.'''\n",
    "\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f30d69",
   "metadata": {},
   "source": [
    "- NNS: sind Plural Nouns -> Regulators, laws, policies. Nomen in der Mehrzahl\n",
    "- VBD: Past tense verbs -> imposed. vergangenheit Verb\n",
    "- PRP$: Possessive Pronomen -> its... , gehört zu App store policies\n",
    "- CD: Cardinal Number -> einfach Zahlen, Numerische Werte! auch ausgeschrieben -> two\n",
    "- NN: Singular Nomen. -> dog, apple fe.\n",
    "- NNP: Proper Noun  Refers to specific names of people, places, organizations, or titles. Proper nouns are always capitalized, like \"Apple\" and \"Tuesday.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.5.0-1025-azure\n",
      "Datetime: 2024-11-26 09:13:33\n",
      "Python Version: 3.11.10\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
